{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model\n",
    "\n",
    "Based on: https://www.kaggle.com/artgor/pytorch-approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import spacy\n",
    "import random\n",
    "import pickle\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow_hub as hub\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added preprocessing from https://www.kaggle.com/wowfattie/3rd-place/data\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', '\\xa0', '\\t',\n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"havent\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"thats\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"theres\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"theyre\":  \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "\n",
    "def clean_data(df, columns: list):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: clean_numbers(x))\n",
    "        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n",
    "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path,'rb') as f:\n",
    "        emb_arr = pickle.load(f)\n",
    "    return emb_arr\n",
    "\n",
    "\n",
    "def build_matrix_adv(embedding_path: str = '',\n",
    "                 embedding_path_spellcheck: str = r'f:\\embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec',\n",
    "                 word_dict: dict = None, lemma_dict: dict = None, max_features: int = 100000,\n",
    "                 embed_size: int= 300, ):\n",
    "    spell_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path_spellcheck)\n",
    "    words = spell_model.index2word\n",
    "    w_rank = {}\n",
    "    for i, word in enumerate(words):\n",
    "        w_rank[word] = i\n",
    "    WORDS = w_rank\n",
    "\n",
    "    def P(word):\n",
    "        \"Probability of `word`.\"\n",
    "        # use inverse of rank as proxy\n",
    "        # returns 0 if the word isn't in the dictionary\n",
    "        return - WORDS.get(word, 0)\n",
    "\n",
    "    def correction(word):\n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return max(candidates(word), key=P)\n",
    "\n",
    "    def candidates(word):\n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (known([word]) or known(edits1(word)) or [word])\n",
    "\n",
    "    def known(words):\n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in WORDS)\n",
    "\n",
    "    def edits1(word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(word):\n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "    def singlify(word):\n",
    "        return \"\".join([letter for i, letter in enumerate(word) if i == 0 or letter != word[i - 1]])\n",
    "\n",
    "\n",
    "    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n",
    "\n",
    "    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8', errors='ignore'))\n",
    "    embedding_index = load_embeddings(embedding_path)\n",
    "\n",
    "    nb_words = min(max_features, len(word_dict))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    unknown_words = []\n",
    "    for word, i in word_dict.items():\n",
    "        key = word\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.lower())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.upper())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.capitalize())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        unknown_words.append(key)\n",
    "\n",
    "    print(f'{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings')\n",
    "    return embedding_matrix, nb_words, unknown_words\n",
    "\n",
    "\n",
    "def get_word_lemma_dict(full_text: list = None, ):\n",
    "    nlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\n",
    "    nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\n",
    "    word_dict = {}\n",
    "    word_index = 1\n",
    "    lemma_dict = {}\n",
    "    docs = nlp.pipe(full_text, n_threads = os.cpu_count())\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n",
    "                word_dict[token.text] = word_index\n",
    "                word_index += 1\n",
    "                lemma_dict[token.text] = token.lemma_\n",
    "\n",
    "    return lemma_dict, word_dict\n",
    "\n",
    "\n",
    "def build_matrix(embedding_path: str = '',\n",
    "                 embedding_path_spellcheck: str = r'f:\\embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec',\n",
    "                 word_dict: dict = None, max_features: int = 100000,\n",
    "                 embed_size: int= 300, ):\n",
    "\n",
    "    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n",
    "    embedding_index = load_embeddings(embedding_path)\n",
    "\n",
    "    nb_words = min(max_features, len(word_dict))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    unknown_words = []\n",
    "    for word, i in word_dict.items():\n",
    "        key = word\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.lower())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.upper())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word.capitalize())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        unknown_words.append(key)\n",
    "\n",
    "    print(f'{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings')\n",
    "    return embedding_matrix, nb_words, unknown_words\n",
    "\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "def fetch_vectors(string_list, batch_size=64, max_len = 512):\n",
    "    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    fin_features = []\n",
    "    for data in chunks(string_list, batch_size):\n",
    "        tokenized = []\n",
    "        for x in data:\n",
    "            x = \" \".join(x.strip().split()[:500])\n",
    "            tok = tokenizer.encode(x, add_special_tokens=True)\n",
    "            tokenized.append(tok[:max_len])\n",
    "\n",
    "        \n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded).to(DEVICE)\n",
    "        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "        fin_features.append(features)\n",
    "\n",
    "    fin_features = np.vstack(fin_features)\n",
    "    return fin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_features(train, test, input_columns, only_test = False, batch_size = 4):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/ragnar123/simple-lgbm-solution-baseline?scriptVersionId=24198335\n",
    "    \"\"\"\n",
    "    \n",
    "    # load universal sentence encoder model to get sentence ambeddings\n",
    "    module_url = \"data/universalsentenceencoderlarge4/\"\n",
    "    embed = hub.load(module_url)\n",
    "    \n",
    "    # create empty dictionaries to store final results\n",
    "    if not only_test:\n",
    "        embedding_train = {}\n",
    "    embedding_test = {}\n",
    "\n",
    "    # iterate over text columns to get senteces embeddings with the previous loaded model\n",
    "    for text in input_columns:\n",
    "    \n",
    "        print(text)\n",
    "        if not only_test:\n",
    "            train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "        test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    \n",
    "        # create empy list to save each batch\n",
    "        curr_train_emb = []\n",
    "        curr_test_emb = []\n",
    "    \n",
    "        # define a batch to transform senteces to their correspinding embedding (1 X 512 for each sentece)\n",
    "        if not only_test:\n",
    "            ind = 0\n",
    "            while ind * batch_size < len(train_text):\n",
    "                curr_train_emb.append(embed(train_text[ind * batch_size: (ind + 1) * batch_size])['outputs'].numpy())\n",
    "                ind += 1\n",
    "        \n",
    "        ind = 0\n",
    "        while ind * batch_size < len(test_text):\n",
    "            curr_test_emb.append(embed(test_text[ind * batch_size: (ind + 1) * batch_size])['outputs'].numpy())\n",
    "            ind += 1\n",
    "\n",
    "        # stack arrays to get a 2D array (dataframe) corresponding with all the sentences and dim 512 for columns (sentence encoder output)\n",
    "        if not only_test:\n",
    "            embedding_train[text + '_embedding'] = np.vstack(curr_train_emb)\n",
    "        embedding_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
    "    \n",
    "    del embed\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    if only_test:\n",
    "        return embedding_test\n",
    "    else:\n",
    "        return embedding_train, embedding_test\n",
    "\n",
    "    \n",
    "def get_dist_features(embedding_train, embedding_test):\n",
    "    \n",
    "    # define a square dist lambda function were (x1 - y1) ^ 2 + (x2 - y2) ^ 2 + (x3 - y3) ^ 2 + ... + (xn - yn) ^ 2\n",
    "    # with this we get one vector of dimension 6079\n",
    "    l2_dist = lambda x, y: np.power(x - y, 2).sum(axis = 1)\n",
    "    \n",
    "    # define a cosine dist lambda function were (x1 * y1) ^ 2 + (x2 * y2) + (x3 * y3) + ... + (xn * yn)\n",
    "    cos_dist = lambda x, y: (x * y).sum(axis = 1)\n",
    "    \n",
    "    # transpose it because we have 6 vector of dimension 6079, need 6079 x 6\n",
    "    dist_features_train = np.array([\n",
    "        l2_dist(embedding_train['question_title_embedding'], embedding_train['answer_embedding']),\n",
    "        l2_dist(embedding_train['question_body_embedding'], embedding_train['answer_embedding']),\n",
    "        l2_dist(embedding_train['question_body_embedding'], embedding_train['question_title_embedding']),\n",
    "        cos_dist(embedding_train['question_title_embedding'], embedding_train['answer_embedding']),\n",
    "        cos_dist(embedding_train['question_body_embedding'], embedding_train['answer_embedding']),\n",
    "        cos_dist(embedding_train['question_body_embedding'], embedding_train['question_title_embedding'])]).T\n",
    "    \n",
    "    # transpose it because we have 6 vector of dimension 6079, need 6079 x 6\n",
    "    dist_features_test = np.array([\n",
    "        l2_dist(embedding_test['question_title_embedding'], embedding_test['answer_embedding']),\n",
    "        l2_dist(embedding_test['question_body_embedding'], embedding_test['answer_embedding']),\n",
    "        l2_dist(embedding_test['question_body_embedding'], embedding_test['question_title_embedding']),\n",
    "        cos_dist(embedding_test['question_title_embedding'], embedding_test['answer_embedding']),\n",
    "        cos_dist(embedding_test['question_body_embedding'], embedding_test['answer_embedding']),\n",
    "        cos_dist(embedding_test['question_body_embedding'], embedding_test['question_title_embedding'])]).T\n",
    "    \n",
    "    return dist_features_train, dist_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and predicting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, n_epochs=3, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    patience = 2\n",
    "\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.1)\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean').cuda()\n",
    "    best_score = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "\n",
    "        for question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, y_batch in tqdm(train_loader, disable=True):\n",
    "            question = question.long().cuda()\n",
    "            answer = answer.long().cuda()\n",
    "            title = title.long().cuda()\n",
    "            category = category.long().cuda()\n",
    "            host = host.long().cuda()\n",
    "            use_emb_q = use_emb_q.cuda()\n",
    "            use_emb_a = use_emb_a.cuda()\n",
    "            use_emb_t = use_emb_t.cuda()\n",
    "            dist_feature = dist_feature.cuda()\n",
    "            \n",
    "            y_batch = y_batch.cuda()\n",
    "            y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature)\n",
    "\n",
    "            loss = loss_fn(y_pred.double(), y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        avg_val_loss = 0.\n",
    "        preds = []\n",
    "        original = []\n",
    "        for i, (question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, y_batch) in enumerate(valid_loader):\n",
    "            question = question.long().cuda()\n",
    "            answer = answer.long().cuda()\n",
    "            title = title.long().cuda()\n",
    "            category = category.long().cuda()\n",
    "            host = host.long().cuda()\n",
    "            use_emb_q = use_emb_q.cuda()\n",
    "            use_emb_a = use_emb_a.cuda()\n",
    "            use_emb_t = use_emb_t.cuda()\n",
    "            dist_feature = dist_feature.cuda()\n",
    "            \n",
    "            y_batch = y_batch.cuda()\n",
    "            y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature).detach()\n",
    "\n",
    "            avg_val_loss += loss_fn(y_pred.double(), y_batch).item() / len(valid_loader)\n",
    "            preds.append(y_pred.cpu().numpy())\n",
    "            original.append(y_batch.cpu().numpy())\n",
    "            \n",
    "        score = 0\n",
    "        for i in range(30):\n",
    "            score += np.nan_to_num(\n",
    "                spearmanr(np.concatenate(original)[:, i], np.concatenate(preds)[:, i]).correlation / 30)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t spearman={:.2f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, score, elapsed_time))\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        valid_score = score\n",
    "        if valid_score > best_score:\n",
    "            best_score = valid_score\n",
    "            p = 0\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "        # check if validation loss didn't improve\n",
    "        if valid_score <= best_score:\n",
    "            p += 1\n",
    "            print(f'{p} epochs of non improving score')\n",
    "            if p > patience:\n",
    "                print('Stopping training')\n",
    "                stop = True\n",
    "                break\n",
    "                \n",
    "        model.load_state_dict(torch.load('model.pt'))\n",
    "                \n",
    "    return model\n",
    "\n",
    "\n",
    "def make_prediction(test_loader: DataLoader = None, model = None):\n",
    "    prediction = np.zeros((len(test_loader.dataset), 30))\n",
    "    model.eval()\n",
    "    for i, (question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, _) in enumerate(test_loader):\n",
    "\n",
    "        start_index = i * test_loader.batch_size\n",
    "        end_index   = min(start_index + test_loader.batch_size, len(test_loader.dataset))\n",
    "        question = question.long().cuda()\n",
    "        answer = answer.long().cuda()\n",
    "        title = title.long().cuda()\n",
    "        category = category.long().cuda()\n",
    "        host = host.long().cuda()\n",
    "        use_emb_q = use_emb_q.cuda()\n",
    "        use_emb_a = use_emb_a.cuda()\n",
    "        use_emb_t = use_emb_t.cuda()\n",
    "        dist_feature = dist_feature.cuda()\n",
    "        y_pred = model(question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature).detach()\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        prediction[start_index:end_index, :] +=  y_pred.detach().cpu().numpy()\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim),\n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Mish(nn.Module):\n",
    "    \"\"\"\n",
    "    Mish - \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\"\n",
    "    https://arxiv.org/abs/1908.08681v1\n",
    "    implemented for PyTorch / FastAI by lessw2020 \n",
    "    github: https://github.com/lessw2020/mish\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)\n",
    "        return x *( torch.tanh(F.softplus(x)))\n",
    "    \n",
    "    \n",
    "    \n",
    "class NeuralNet5(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int = 128,\n",
    "                 max_len: int = 500,\n",
    "                 max_len_title: int = 30,\n",
    "                 n_cat: int = 3,\n",
    "                 cat_emb: int = 6,\n",
    "                 n_host: int = 55,\n",
    "                 host_emb: int = 28,\n",
    "                 additional_embedding_shape: int = 512,\n",
    "                 embedding_matrix=None):\n",
    "        super(NeuralNet5, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.category_embedding = nn.Embedding(n_cat, int(cat_emb))\n",
    "        self.host_embedding = nn.Embedding(n_host, int(host_emb))\n",
    "\n",
    "        self.linear_q_add = nn.Linear(300, 128)\n",
    "        self.linear_q_add1 = nn.Linear(128, 30)\n",
    "        self.bilinear_add = nn.Bilinear(30, 30, 30)\n",
    "\n",
    "        self.lstm_q = nn.LSTM(300, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru_q = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm_a = nn.LSTM(300, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru_a = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm_t = nn.LSTM(300, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru_t = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.lstm_attention_q = Attention(hidden_size * 2, max_len)\n",
    "        self.gru_attention_q = Attention(hidden_size * 2, max_len)\n",
    "\n",
    "        self.lstm_attention_a = Attention(hidden_size * 2, max_len)\n",
    "        self.gru_attention_a = Attention(hidden_size * 2, max_len)\n",
    "\n",
    "        self.lstm_attention_t = Attention(hidden_size * 2, max_len_title)\n",
    "        self.gru_attention_t = Attention(hidden_size * 2, max_len_title)\n",
    "\n",
    "        self.linear_q = nn.Linear(1024, 64)\n",
    "        self.relu_q = Mish()\n",
    "\n",
    "        self.linear_a = nn.Linear(1024, 64)\n",
    "        self.relu_a = Mish()\n",
    "\n",
    "        self.linear_t = nn.Linear(1024, 64)\n",
    "        self.relu_t = Mish()\n",
    "        \n",
    "        self.linear_q_emb = nn.Linear(additional_embedding_shape, 64)\n",
    "        self.relu_q_emb = Mish()\n",
    "\n",
    "        self.linear_a_emb = nn.Linear(additional_embedding_shape, 64)\n",
    "        self.relu_a_emb = Mish()\n",
    "\n",
    "        self.linear_t_emb = nn.Linear(additional_embedding_shape, 64)\n",
    "        self.relu_t_emb = Mish()\n",
    "\n",
    "        self.linear1 = nn.Sequential(nn.Linear(256 + int(cat_emb) + int(host_emb) + 6, 64),\n",
    "                                     nn.BatchNorm1d(64),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.Dropout(0.5))\n",
    "\n",
    "        self.linear_q_out = nn.Linear(64, 21)\n",
    "\n",
    "        self.bilinear = nn.Bilinear(64, 64, 64)\n",
    "        self.bilinear_emb = nn.Bilinear(64, 64, 64)\n",
    "        self.linear2 = nn.Sequential(nn.Linear(390, 64),\n",
    "                                     nn.BatchNorm1d(64),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.Dropout(0.5))\n",
    "\n",
    "        self.linear_aq_out = nn.Linear(64, 9)\n",
    "\n",
    "    def forward(self, question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature):\n",
    "        h_embedding_q = self.embedding(question)\n",
    "        h_embedding_q = self.embedding_dropout(h_embedding_q)\n",
    "\n",
    "        h_lstm_q, _ = self.lstm_q(h_embedding_q)\n",
    "        h_gru_q, _ = self.gru_q(h_lstm_q)\n",
    "\n",
    "        h_lstm_atten_q = self.lstm_attention_q(h_lstm_q)\n",
    "        h_gru_atten_q = self.gru_attention_q(h_gru_q)\n",
    "\n",
    "        avg_pool_q = torch.mean(h_gru_q, 1)\n",
    "        max_pool_q, _ = torch.max(h_gru_q, 1)\n",
    "\n",
    "        h_embedding_a = self.embedding(answer)\n",
    "        h_embedding_a = self.embedding_dropout(h_embedding_a)\n",
    "\n",
    "        h_lstm_a, _ = self.lstm_a(h_embedding_a)\n",
    "        h_gru_a, _ = self.gru_a(h_lstm_a)\n",
    "\n",
    "        h_lstm_atten_a = self.lstm_attention_a(h_lstm_a)\n",
    "        h_gru_atten_a = self.gru_attention_a(h_gru_a)\n",
    "\n",
    "        avg_pool_a = torch.mean(h_gru_a, 1)\n",
    "        max_pool_a, _ = torch.max(h_gru_a, 1)\n",
    "\n",
    "        h_embedding_t = self.embedding(title)\n",
    "        h_embedding_t = self.embedding_dropout(h_embedding_t)\n",
    "\n",
    "        h_lstm_t, _ = self.lstm_t(h_embedding_t)\n",
    "        h_gru_t, _ = self.gru_t(h_lstm_t)\n",
    "\n",
    "        h_lstm_atten_t = self.lstm_attention_t(h_lstm_t)\n",
    "        h_gru_atten_t = self.gru_attention_t(h_gru_t)\n",
    "\n",
    "        avg_pool_t = torch.mean(h_gru_t, 1)\n",
    "        max_pool_t, _ = torch.max(h_gru_t, 1)\n",
    "\n",
    "        category = self.category_embedding(category)\n",
    "        host = self.host_embedding(host)\n",
    "        \n",
    "        add = torch.cat((h_embedding_q, h_embedding_a, h_embedding_t), 1)\n",
    "        add = self.linear_q_add(torch.mean(add, 1))\n",
    "        add = self.linear_q_add1(add)\n",
    "\n",
    "        q = torch.cat((h_lstm_atten_q, h_gru_atten_q, avg_pool_q, max_pool_q), 1)\n",
    "        a = torch.cat((h_lstm_atten_a, h_gru_atten_a, avg_pool_a, max_pool_a), 1)\n",
    "        t = torch.cat((h_lstm_atten_t, h_gru_atten_t, avg_pool_t, max_pool_t), 1)\n",
    "        \n",
    "        q = self.relu_q(self.linear_q(q))\n",
    "        a = self.relu_a(self.linear_a(a))\n",
    "        t = self.relu_t(self.linear_t(t))\n",
    "\n",
    "        q_emb = self.relu_q_emb(self.linear_q_emb(use_emb_q))\n",
    "        a_emb = self.relu_a_emb(self.linear_a_emb(use_emb_a))\n",
    "        t_emb = self.relu_t_emb(self.linear_t_emb(use_emb_t))\n",
    "        \n",
    "        hidden_q = self.linear1(torch.cat((q, t, q_emb, t_emb, category, host, dist_feature), 1))\n",
    "        q_result = self.linear_q_out(hidden_q)\n",
    "\n",
    "        bil_sim = self.bilinear(q, a)\n",
    "        bil_sim_emb = self.bilinear_emb(q_emb, a_emb)\n",
    "        \n",
    "        hidden_aq = self.linear2(torch.cat((q, a, q_emb, a_emb, bil_sim, bil_sim_emb, dist_feature), 1))\n",
    "        aq_result = self.linear_aq_out(hidden_aq)\n",
    "\n",
    "        out = torch.cat([q_result, aq_result], 1)\n",
    "        out = self.bilinear_add(out, add)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, question_data, answer_data, title_data, category_data, host_data, use_embeddings, dist_features, idxs, targets=None):\n",
    "        self.question_data = question_data[idxs]\n",
    "        self.answer_data = answer_data[idxs]\n",
    "        self.title_data = title_data[idxs]\n",
    "        self.category_data = category_data[idxs]\n",
    "        self.host_data = host_data[idxs]\n",
    "        self.use_embeddings_q = use_embeddings['question_body_embedding'][idxs]\n",
    "        self.use_embeddings_a = use_embeddings['answer_embedding'][idxs]\n",
    "        self.use_embeddings_t = use_embeddings['question_title_embedding'][idxs]\n",
    "        self.dist_features = dist_features[idxs]\n",
    "        self.targets = targets[idxs] if targets is not None else np.zeros((self.question_data.shape[0], 30))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.question_data[idx]\n",
    "        answer = self.answer_data[idx]\n",
    "        title = self.title_data[idx]\n",
    "        category = self.category_data[idx]\n",
    "        host = self.host_data[idx]\n",
    "        use_emb_q = self.use_embeddings_q[idx]\n",
    "        use_emb_a = self.use_embeddings_a[idx]\n",
    "        use_emb_t = self.use_embeddings_t[idx]\n",
    "        dist_feature = self.dist_features[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        return question, answer, title, category, host, use_emb_q, use_emb_a, use_emb_t, dist_feature, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.question_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 10)\n",
    "pd.set_option('max_columns', 500)\n",
    "path = 'data'\n",
    "sample_submission = pd.read_csv(f'{path}/sample_submission.csv')\n",
    "test = pd.read_csv(f'{path}/test.csv').fillna(' ')\n",
    "train = pd.read_csv(f'{path}/train.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO, do we really want this?\n",
    "train = clean_data(train, ['answer', 'question_body', 'question_title'])\n",
    "test = clean_data(test, ['answer', 'question_body', 'question_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\n",
      "question_body\n",
      "question_title\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "embedding_test = get_embedding_features(train, test, ['answer', 'question_body', 'question_title'], only_test=True)\n",
    "embedding_train = {}\n",
    "embedding_train['answer_embedding'] = np.load('data/qa-labeling-files-for-inference/embedding_train_answer_embedding.npy', allow_pickle=True)\n",
    "embedding_train['question_body_embedding'] = np.load('data/qa-labeling-files-for-inference/embedding_train_question_body_embedding.npy', allow_pickle=True)\n",
    "embedding_train['question_title_embedding'] = np.load('data/qa-labeling-files-for-inference/embedding_train_question_title_embedding.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 158 ms, sys: 32 µs, total: 158 ms\n",
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dist_features_train, dist_features_test  = get_dist_features(embedding_train, embedding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7974    , 0.4514755 , 0.78082395, 0.6013001 , 0.7742623 ,\n",
       "        0.60958815],\n",
       "       [1.1588576 , 0.61425245, 0.90267396, 0.42057133, 0.6928739 ,\n",
       "        0.5486632 ],\n",
       "       [1.382694  , 0.9097824 , 1.2391388 , 0.30865306, 0.5451089 ,\n",
       "        0.38043067],\n",
       "       ...,\n",
       "       [1.351615  , 1.3787203 , 0.60093856, 0.32419258, 0.31063998,\n",
       "        0.69953084],\n",
       "       [1.3469465 , 0.55700326, 1.1053724 , 0.32652682, 0.72149837,\n",
       "        0.44731393],\n",
       "       [1.165278  , 1.0739144 , 0.5959264 , 0.4173611 , 0.4630429 ,\n",
       "        0.70203686]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "full_text = list(train['question_body']) + \\\n",
    "                       list(train['answer']) + \\\n",
    "                       list(train['question_title']) + \\\n",
    "                       list(test['question_body']) + \\\n",
    "                       list(test['answer']) + \\\n",
    "                       list(test['question_title'])\n",
    "tokenizer.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=300\n",
    "embedding_path = \"data/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_dict, word_dict = get_word_lemma_dict(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.7148% words are not in embeddings\n",
      "CPU times: user 4.6 s, sys: 1.19 s, total: 5.79 s\n",
      "Wall time: 5.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_matrix, nb_words, unknown_words = build_matrix(embedding_path, 'data/wikinews300d1mvec/wiki-news-300d-1M.vec', tokenizer.word_index,\n",
    "                                              100000, embed_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tk.word_index = {k: v for k, v in tk.word_index.items() if k in word_dict.keys()}\n",
    "# train['host'] = train['host'].apply(lambda x: x.split('.')[-2])\n",
    "# test['host'] = test['host'].apply(lambda x: x.split('.')[-2])\n",
    "unique_hosts = list(set(train['host'].unique().tolist() + test['host'].unique().tolist()))\n",
    "host_dict = {i + 1: e for i, e in enumerate(unique_hosts)}\n",
    "host_dict_reverse = {v: k for k, v in host_dict.items()}\n",
    "\n",
    "unique_categories = list(set(train['category'].unique().tolist() + test['category'].unique().tolist()))\n",
    "category_dict = {i + 1: e for i, e in enumerate(unique_categories)}\n",
    "category_dict_reverse = {v: k for k, v in category_dict.items()}\n",
    "max_len = 500             # TODO: Is this appropriate\n",
    "max_len_title = 30\n",
    "train_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_body']), maxlen = max_len)\n",
    "train_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['answer']), maxlen = max_len)\n",
    "train_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_title']), maxlen = max_len_title)\n",
    "\n",
    "test_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['question_body']), maxlen = max_len)\n",
    "test_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['answer']), maxlen = max_len)\n",
    "test_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['question_title']), maxlen = max_len_title)\n",
    "\n",
    "train_host = train['host'].apply(lambda x: host_dict_reverse[x]).values\n",
    "train_category = train['category'].apply(lambda x: category_dict_reverse[x]).values\n",
    "\n",
    "test_host = test['host'].apply(lambda x: host_dict_reverse[x]).values\n",
    "test_category = test['category'].apply(lambda x: category_dict_reverse[x]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[sample_submission.columns[1:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "bs = 8\n",
    "n_cat = len(category_dict) + 1\n",
    "cat_emb = min(np.ceil((len(category_dict)) / 2), 50)\n",
    "n_host = len(host_dict)+1\n",
    "host_emb = min(np.ceil((len(host_dict)) / 2), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_test = 16\n",
    "test_loader = DataLoader(TextDataset(test_question_tokenized, test_answer_tokenized, test_title_tokenized,\n",
    "                                     test_category, test_host, embedding_test, dist_features_test, test.index),\n",
    "                          batch_size=bs_test, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Mon Jan 13 14:54:42 2020\n",
      "Epoch 1/3 \t loss=0.4237 \t val_loss=0.3938 \t spearman=0.28 \t time=63.00s\n",
      "1 epochs of non improving score\n",
      "Epoch 2/3 \t loss=0.3955 \t val_loss=0.3869 \t spearman=0.32 \t time=61.23s\n",
      "1 epochs of non improving score\n",
      "Epoch 3/3 \t loss=0.3877 \t val_loss=0.3848 \t spearman=0.33 \t time=60.96s\n",
      "1 epochs of non improving score\n",
      "\n",
      "Fold 2 started at Mon Jan 13 14:57:49 2020\n",
      "Epoch 1/3 \t loss=0.4244 \t val_loss=0.3951 \t spearman=0.28 \t time=62.08s\n",
      "1 epochs of non improving score\n",
      "Epoch 2/3 \t loss=0.3977 \t val_loss=0.3886 \t spearman=0.30 \t time=61.39s\n",
      "1 epochs of non improving score\n",
      "Epoch 3/3 \t loss=0.3890 \t val_loss=0.3840 \t spearman=0.32 \t time=62.13s\n",
      "1 epochs of non improving score\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=2, random_state=42)\n",
    "preds = np.zeros((len(test), 30))\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(train)):\n",
    "    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "    train_loader = DataLoader(TextDataset(train_question_tokenized, train_answer_tokenized, train_title_tokenized, train_category, train_host, embedding_train,\n",
    "                                          dist_features_train, train_index, y),\n",
    "                              batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    valid_loader = DataLoader(TextDataset(train_question_tokenized, train_answer_tokenized, train_title_tokenized, train_category, train_host, embedding_train,\n",
    "                                          dist_features_train, valid_index, y),\n",
    "                              batch_size=bs, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "    model = NeuralNet5(embedding_matrix=embedding_matrix,\n",
    "                       n_cat=n_cat,\n",
    "                       cat_emb=cat_emb,\n",
    "                       n_host=n_host,\n",
    "                       host_emb=host_emb)\n",
    "    model.cuda()\n",
    "\n",
    "    model = train_model(model, train_loader, valid_loader, n_epochs=3, lr=0.001)\n",
    "    prediction = make_prediction(test_loader, model)\n",
    "    preds += prediction / folds.n_splits / 2\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipping is necessary or we will get an error\n",
    "sample_submission.loc[:, 'question_asker_intent_understanding':] = np.clip(preds, 0.00001, 0.999999)\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
